"""
flow_train.py
-------------
Train a Masked Autoregressive Flow (MAF) on the simulated training data.

The flow learns the conditional density p(θ|x), enabling fast posterior
sampling once trained.

Usage:
    python flow_train.py
    # or via CLI: hubble train
"""

import torch
from torch.utils.data import DataLoader, TensorDataset

from models import build_flow, save_flow
from config import config, paths, DEVICE, logger, set_seed


def load_training_data() -> tuple[torch.Tensor, torch.Tensor]:
    """
    Load the training data generated by sim.py.

    Returns
    -------
    tuple[torch.Tensor, torch.Tensor]
        (x, θ) tensors - summary vectors and parameters.
    """
    if not paths.train_data.exists():
        raise FileNotFoundError(
            f"Training data not found at {paths.train_data}. "
            "Run 'python sim.py' first."
        )

    logger.info(f"Loading training data from {paths.train_data}")
    data = torch.load(paths.train_data)

    θ = data["θ"]
    x = data["x"]

    logger.info(f"  Loaded {len(θ):,} training samples")
    logger.info(f"  θ shape: {θ.shape}, x shape: {x.shape}")

    return x, θ


def train_flow(
    x: torch.Tensor,
    θ: torch.Tensor,
    n_epochs: int = None,
    batch_size: int = None,
    learning_rate: float = None,
) -> None:
    """
    Train the normalizing flow on the provided data.

    Parameters
    ----------
    x : torch.Tensor
        Summary vectors (context), shape (N, D_x).
    θ : torch.Tensor
        Parameters (targets), shape (N, 5).
    n_epochs : int, optional
        Number of training epochs. Default from config.
    batch_size : int, optional
        Batch size. Default from config.
    learning_rate : float, optional
        Learning rate. Default from config.
    """
    n_epochs = n_epochs or config.training.n_epochs
    batch_size = batch_size or config.training.batch_size
    learning_rate = learning_rate or config.training.learning_rate

    logger.info("Building flow model...")
    flow = build_flow()

    n_params = sum(p.numel() for p in flow.parameters())
    logger.info(f"  Model has {n_params:,} parameters")

    # Create data loader
    loader = DataLoader(
        TensorDataset(x, θ),
        batch_size=batch_size,
        shuffle=True
    )

    optimizer = torch.optim.Adam(flow.parameters(), lr=learning_rate)

    logger.info(f"Training for {n_epochs} epochs...")
    logger.info(f"  Batch size: {batch_size}")
    logger.info(f"  Learning rate: {learning_rate}")

    best_loss = float("inf")

    for epoch in range(n_epochs):
        epoch_loss = 0.0
        n_batches = 0

        for xb, θb in loader:
            # Negative log-likelihood loss
            loss = -flow.log_prob(inputs=θb, context=xb).mean()

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            epoch_loss += loss.item()
            n_batches += 1

        avg_loss = epoch_loss / n_batches
        if avg_loss < best_loss:
            best_loss = avg_loss
            marker = " *"
        else:
            marker = ""

        logger.info(f"  Epoch {epoch + 1:2d}/{n_epochs}  loss: {avg_loss:.4f}{marker}")

    # Save with metadata
    metadata = {
        "n_epochs": n_epochs,
        "batch_size": batch_size,
        "learning_rate": learning_rate,
        "final_loss": avg_loss,
        "best_loss": best_loss,
        "n_training_samples": len(θ),
    }
    save_flow(flow, metadata)


def run() -> None:
    """Run the full training pipeline."""
    set_seed()

    logger.info("=" * 60)
    logger.info("Starting flow training")
    logger.info("=" * 60)

    x, θ = load_training_data()
    train_flow(x, θ)

    logger.info("Flow training complete!")


if __name__ == "__main__":
    run()
